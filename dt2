import requests
import pandas as pd

# API details
BASE_API_URL = "https://dependencytrackapi.crm.com/api/v1"
API_TOKEN = "bzbdbdhdhd"
HEADERS = {
    "X-Api-Key": API_TOKEN,
    "Content-Type": "application/json",
    "Accept": "application/json"
}

INPUT_FILE = "dependency_track.xlsx"  # First Excel file containing UUIDs
OUTPUT_FILE = "DTResult.xlsx"  # Final filtered result file

def fetch_all_findings(uuid):
    """Fetch all pages of findings for a given project UUID."""
    all_findings = []
    page = 1
    limit = 100  # Number of records per request

    while True:
        url = f"{BASE_API_URL}/finding/project/{uuid}"
        params = {"page": page, "limit": limit}  # Pagination parameters
        
        try:
            response = requests.get(url, headers=HEADERS, params=params)
            response.raise_for_status()
            data = response.json()

            if not data:
                break  # Stop if no more records

            all_findings.extend(data)  # Append current batch to full list

            print(f"Fetched {len(data)} records from page {page} for UUID {uuid}")

            # Stop if fewer than 'limit' records are returned (last page)
            if len(data) < limit:
                break

            page += 1  # Move to next page
        except requests.exceptions.RequestException as e:
            print(f"Error fetching findings for UUID {uuid}: {e}")
            break

    return all_findings

def process_and_save_findings():
    """Read UUIDs from Excel, fetch findings, filter, and save to Excel."""
    # Load UUIDs from the first Excel file
    df_uuids = pd.read_excel(INPUT_FILE, usecols=["uuid"])
    all_results = []

    for uuid in df_uuids["uuid"]:
        findings = fetch_all_findings(uuid)
        if not findings:
            continue  # Skip if no data found
        
        df_findings = pd.json_normalize(findings)  # Flatten JSON

        # Keep only 'component.project' and 'analysis.state' columns
        required_columns = ["component.project", "analysis.state"]
        df_findings = df_findings[required_columns]

        all_results.append(df_findings)

    # Combine all data into a single DataFrame
    if all_results:
        final_df = pd.concat(all_results, ignore_index=True)
        final_df.to_excel(OUTPUT_FILE, index=False)
        print(f"Filtered findings saved to {OUTPUT_FILE}")
    else:
        print("No data found for any UUID.")

if __name__ == "__main__":
    process_and_save_findings()