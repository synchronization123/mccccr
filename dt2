import os
import requests
import pandas as pd
from glob import glob

# API details
BASE_API_URL = "https://dependencytrackapi.crm.com/api/v1"
API_TOKEN = "bzbdbdhdhd"
HEADERS = {
    "X-Api-Key": API_TOKEN,
    "Content-Type": "application/json",
    "Accept": "application/json"
}

INPUT_FILE = "dependency_track.xlsx"  # Input Excel file containing UUIDs
OUTPUT_FOLDER = "data/output"  # Folder to store individual Excel files
FINAL_OUTPUT_FILE = "DTResult.xlsx"  # Merged final Excel file

# Ensure output folder exists
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# Required columns
REQUIRED_COLUMNS = ["component.project", "analysis.state"]

def fetch_all_findings(uuid):
    """Fetch all findings for a given project UUID with pagination."""
    all_findings = []
    page = 1
    limit = 100  # API limit per request
    max_pages = 100  # Fail-safe: Stop after 100 pages

    while page <= max_pages:
        url = f"{BASE_API_URL}/finding/project/{uuid}"
        params = {"page": page, "limit": limit}  # Pagination parameters

        try:
            response = requests.get(url, headers=HEADERS, params=params)
            response.raise_for_status()
            data = response.json()

            if not data:
                print(f"No more data for UUID {uuid}, stopping pagination.")
                break  # Stop if no more records

            all_findings.extend(data)  # Append current batch to full list

            print(f"Fetched {len(data)} records from page {page} for UUID {uuid}")

            if len(data) < limit:
                print(f"Last page reached for UUID {uuid}.")
                break  # Stop if fewer than 'limit' records are returned

            page += 1  # Move to next page
        except requests.exceptions.RequestException as e:
            print(f"Error fetching findings for UUID {uuid}: {e}")
            break

    return all_findings

def process_and_save_findings():
    """Read UUIDs from Excel, fetch findings, filter, and save each to separate Excel files."""
    try:
        # Load UUIDs from the first Excel file
        df_uuids = pd.read_excel(INPUT_FILE, usecols=["uuid"])
    except Exception as e:
        print(f"Error reading UUIDs from Excel: {e}")
        return

    for uuid in df_uuids["uuid"]:
        findings = fetch_all_findings(uuid)
        
        if findings:
            df_findings = pd.json_normalize(findings)  # Flatten JSON
        else:
            # Create an empty DataFrame with required columns
            df_findings = pd.DataFrame(columns=REQUIRED_COLUMNS)  

        # Ensure only required columns exist
        if "component.project" not in df_findings.columns:
            df_findings["component.project"] = "Analysis Pending"  # Add column if missing
        if "analysis.state" not in df_findings.columns:
            df_findings["analysis.state"] = "Analysis Pending"  # Add column if missing

        # Fill missing values with 'Analysis Pending'
        df_findings[REQUIRED_COLUMNS] = df_findings[REQUIRED_COLUMNS].fillna("Analysis Pending")

        # Save individual UUID data to an Excel file
        file_path = os.path.join(OUTPUT_FOLDER, f"{uuid}.xlsx")
        df_findings.to_excel(file_path, index=False)
        print(f"Saved findings for UUID {uuid} to {file_path}")

def merge_and_cleanup():
    """Merge all Excel files in the output folder (keeping only required columns) and delete individual files."""
    files = glob(os.path.join(OUTPUT_FOLDER, "*.xlsx"))

    if not files:
        print("No files found to merge.")
        return

    all_dfs = []
    for file in files:
        df = pd.read_excel(file, usecols=REQUIRED_COLUMNS)  # Load only required columns
        all_dfs.append(df)

    if all_dfs:
        merged_df = pd.concat(all_dfs, ignore_index=True)
        merged_df.to_excel(FINAL_OUTPUT_FILE, index=False)
        print(f"Merged data saved to {FINAL_OUTPUT_FILE}")

        # Delete individual files
        for file in files:
            os.remove(file)
        print("Deleted individual UUID Excel files.")

if __name__ == "__main__":
    process_and_save_findings()
    merge_and_cleanup()